{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T22:03:54.987829Z",
     "start_time": "2024-11-19T22:03:54.978357Z"
    }
   },
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from diff_attn import DifferentialAttention"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T22:03:57.883760Z",
     "start_time": "2024-11-19T22:03:57.863012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diff_attn import DifferentialAttention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TranslationEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_head, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'attention': DifferentialAttention(d_model, d_head, n_heads, dropout),\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, 4 * d_model),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4 * d_model, d_model)\n",
    "                ),\n",
    "                'norm2': nn.LayerNorm(d_model)\n",
    "            }) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.embedding(x) * math.sqrt(x.size(-1))  # [batch_size, seq_len, d_model]\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # Self-attention\n",
    "            attn_out = layer['attention'](x, mask=mask)\n",
    "            x = layer['norm1'](x + self.dropout(attn_out))\n",
    "            \n",
    "            # Feed forward\n",
    "            ffn_out = layer['ffn'](x)\n",
    "            x = layer['norm2'](x + self.dropout(ffn_out))\n",
    "            \n",
    "        return x\n",
    "\n",
    "class TranslationDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_head, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'self_attention': DifferentialAttention(d_model, d_head, n_heads, dropout),\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'cross_attention': DifferentialAttention(d_model, d_head, n_heads, dropout),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, 4 * d_model),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4 * d_model, d_model)\n",
    "                ),\n",
    "                'norm3': nn.LayerNorm(d_model)\n",
    "            }) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.embedding(x) * math.sqrt(x.size(-1))  # [batch_size, seq_len, d_model]\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # Self-attention\n",
    "            self_attn = layer['self_attention'](x, mask=tgt_mask)\n",
    "            x = layer['norm1'](x + self.dropout(self_attn))\n",
    "            \n",
    "            # Cross-attention\n",
    "            cross_attn = layer['cross_attention'](x, context=encoder_output, mask=src_mask)\n",
    "            x = layer['norm2'](x + self.dropout(cross_attn))\n",
    "            \n",
    "            # Feed forward\n",
    "            ffn_out = layer['ffn'](x)\n",
    "            x = layer['norm3'](x + self.dropout(ffn_out))\n",
    "\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, d_head=64, n_heads=8, \n",
    "                 n_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TranslationEncoder(src_vocab_size, d_model, d_head, n_heads, n_layers, dropout)\n",
    "        self.decoder = TranslationDecoder(tgt_vocab_size, d_model, d_head, n_heads, n_layers, dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # src shape: [batch_size, src_seq_len]\n",
    "        # tgt shape: [batch_size, tgt_seq_len]\n",
    "        \n",
    "        encoder_output = self.encoder(src, src_mask)  # [batch_size, src_seq_len, d_model]\n",
    "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)  # [batch_size, tgt_seq_len, tgt_vocab_size]\n",
    "        \n",
    "        return decoder_output\n",
    "\n",
    "def create_masks(src, tgt, pad_idx):\n",
    "    # Source mask\n",
    "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # Target mask\n",
    "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # Causal mask for decoder\n",
    "    seq_len = tgt.size(1)\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    causal_mask = causal_mask.unsqueeze(0)\n",
    "    \n",
    "    tgt_mask = tgt_mask & ~causal_mask\n",
    "    \n",
    "    return src_mask, tgt_mask"
   ],
   "id": "87ddbd65d105f625",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T22:13:30.593427Z",
     "start_time": "2024-11-19T22:10:54.686433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        # WMT14 dataset has a 'translation' field containing source and target text\n",
    "        source_text = item['translation']['en']  # English text\n",
    "        target_text = item['translation']['cs']  # Czech text\n",
    "\n",
    "        # Tokenize inputs\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Tokenize targets\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': source_encoding['attention_mask'].squeeze(),\n",
    "            'labels': target_encoding['input_ids'].squeeze(),\n",
    "            'decoder_attention_mask': target_encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def train_model():\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"translation-differential-attention\")\n",
    "\n",
    "    # Model parameters\n",
    "    max_length = 128\n",
    "    batch_size = 16\n",
    "    num_epochs = 3\n",
    "    learning_rate = 2e-5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"wmt14\", \"cs-en\")  # Explicitly specify language pair\n",
    "    \n",
    "    # Take a subset of the data for faster training\n",
    "    train_size = 100000  # Adjust this number based on your computational resources\n",
    "    \n",
    "    # Create smaller training and validation sets\n",
    "    train_dataset = dataset['train'].select(range(train_size))\n",
    "    val_dataset = dataset['validation'].select(range(1000))  # smaller validation set\n",
    "\n",
    "    print(f\"Training on {len(train_dataset)} examples\")\n",
    "    print(f\"Validating on {len(val_dataset)} examples\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-cs\")  # Changed to en-cs tokenizer\n",
    "\n",
    "    # Create custom datasets\n",
    "    train_data = TranslationDataset(train_dataset, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_data = TranslationDataset(val_dataset, tokenizer, max_length)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model = TranslationModel(\n",
    "        src_vocab_size=tokenizer.vocab_size,\n",
    "        tgt_vocab_size=tokenizer.vocab_size,\n",
    "        d_model=512,\n",
    "        d_head=64,\n",
    "        n_heads=8,\n",
    "        n_layers=6,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n",
    "\n",
    "            # Create masks\n",
    "            src_mask, tgt_mask = create_masks(input_ids, labels, tokenizer.pad_token_id)\n",
    "            src_mask = src_mask.to(device)\n",
    "            tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, labels[:, :-1], src_mask, tgt_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fct(outputs.view(-1, outputs.size(-1)), labels[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'learning_rate': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "\n",
    "            # Save checkpoint every 5000 batches\n",
    "            if batch_idx % 5000 == 0:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }\n",
    "                torch.save(checkpoint, f'checkpoint_epoch_{epoch}_batch_{batch_idx}.pt')\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Average training loss: {avg_train_loss}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n",
    "\n",
    "                src_mask, tgt_mask = create_masks(input_ids, labels, tokenizer.pad_token_id)\n",
    "                src_mask = src_mask.to(device)\n",
    "                tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "                outputs = model(input_ids, labels[:, :-1], src_mask, tgt_mask)\n",
    "                loss = loss_fct(outputs.view(-1, outputs.size(-1)), labels[:, 1:].contiguous().view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Validation loss: {avg_val_loss}')\n",
    "        wandb.log({'val_loss': avg_val_loss})\n",
    "\n",
    "        # Save epoch checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_train_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    wandb.finish()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Function to translate text\n",
    "def translate(model, tokenizer, text, max_length=128):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Create start token for decoder\n",
    "    start_token = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate translation one token at a time\n",
    "        generated_ids = start_token\n",
    "        for _ in range(max_length):\n",
    "            src_mask, tgt_mask = create_masks(input_ids, generated_ids, tokenizer.pad_token_id)\n",
    "            src_mask = src_mask.to(device)\n",
    "            tgt_mask = tgt_mask.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, generated_ids, src_mask, tgt_mask)\n",
    "            next_token = outputs[:, -1:].argmax(dim=-1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Training the model\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model, tokenizer = train_model()\n",
    "        \n",
    "        # Test translation\n",
    "        test_sentence = \"Hello, how are you?\"\n",
    "        translation = translate(model, tokenizer, test_sentence)\n",
    "        print(f\"Source: {test_sentence}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        wandb.finish()"
   ],
   "id": "ef7a0093c0cb1fba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011173725921738271, max=1.0â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "422582e160af4843be2aa436f3164fbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/stevemama/diff-transformer/module/wandb/run-20241119_171054-vtf1yhu2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevemama/translation-differential-attention/runs/vtf1yhu2' target=\"_blank\">crimson-shape-5</a></strong> to <a href='https://wandb.ai/stevemama/translation-differential-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/stevemama/translation-differential-attention' target=\"_blank\">https://wandb.ai/stevemama/translation-differential-attention</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/stevemama/translation-differential-attention/runs/vtf1yhu2' target=\"_blank\">https://wandb.ai/stevemama/translation-differential-attention/runs/vtf1yhu2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data:   0%|          | 0.00/168M [00:00<?, ?B/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 227\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 227\u001B[0m         model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m         \u001B[38;5;66;03m# Test translation\u001B[39;00m\n\u001B[1;32m    230\u001B[0m         test_sentence \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, how are you?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[0;32mIn[8], line 62\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# Load dataset\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading dataset...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 62\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwmt14\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcs-en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Explicitly specify language pair\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Take a subset of the data for faster training\u001B[39;00m\n\u001B[1;32m     65\u001B[0m train_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100000\u001B[39m  \u001B[38;5;66;03m# Adjust this number based on your computational resources\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/load.py:2582\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2579\u001B[0m try_from_hf_gcs \u001B[38;5;241m=\u001B[39m path \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES\n\u001B[1;32m   2581\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 2582\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2583\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2584\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2585\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2586\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtry_from_hf_gcs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtry_from_hf_gcs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2587\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2588\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2589\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2591\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   2592\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2593\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   2594\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/builder.py:1005\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m   1003\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1004\u001B[0m         prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m-> 1005\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m   1012\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/builder.py:1078\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m   1076\u001B[0m split_dict \u001B[38;5;241m=\u001B[39m SplitDict(dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_name)\n\u001B[1;32m   1077\u001B[0m split_generators_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001B[0;32m-> 1078\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_generators\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msplit_generators_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Checksums verification\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS \u001B[38;5;129;01mand\u001B[39;00m dl_manager\u001B[38;5;241m.\u001B[39mrecord_checksums:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:43\u001B[0m, in \u001B[0;36mParquet._split_generators\u001B[0;34m(self, dl_manager)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdata_files:\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one data file must be specified, but got data_files=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdata_files\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 43\u001B[0m data_files \u001B[38;5;241m=\u001B[39m \u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_extract\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_files, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m     45\u001B[0m     files \u001B[38;5;241m=\u001B[39m data_files\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/download/download_manager.py:570\u001B[0m, in \u001B[0;36mDownloadManager.download_and_extract\u001B[0;34m(self, url_or_urls)\u001B[0m\n\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdownload_and_extract\u001B[39m(\u001B[38;5;28mself\u001B[39m, url_or_urls):\n\u001B[1;32m    555\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001B[39;00m\n\u001B[1;32m    556\u001B[0m \n\u001B[1;32m    557\u001B[0m \u001B[38;5;124;03m    Is roughly equivalent to:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001B[39;00m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 570\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextract(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl_or_urls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/download/download_manager.py:434\u001B[0m, in \u001B[0;36mDownloadManager.download\u001B[0;34m(self, url_or_urls)\u001B[0m\n\u001B[1;32m    432\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m stack_multiprocessing_download_progress_bars():\n\u001B[0;32m--> 434\u001B[0m     downloaded_path_or_paths \u001B[38;5;241m=\u001B[39m \u001B[43mmap_nested\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_urls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmap_tuple\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDownloading data files\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m duration \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    442\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading took \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mduration\u001B[38;5;241m.\u001B[39mtotal_seconds()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m60\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m min\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:475\u001B[0m, in \u001B[0;36mmap_nested\u001B[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001B[0m\n\u001B[1;32m    464\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    465\u001B[0m         map_nested(\n\u001B[1;32m    466\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m    473\u001B[0m     ]\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m num_proc \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m num_proc \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m<\u001B[39m parallel_min_length:\n\u001B[0;32m--> 475\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    476\u001B[0m         _single_map_nested((function, obj, types, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    477\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m hf_tqdm(iterable, disable\u001B[38;5;241m=\u001B[39mdisable_tqdm, desc\u001B[38;5;241m=\u001B[39mdesc)\n\u001B[1;32m    478\u001B[0m     ]\n\u001B[1;32m    479\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    480\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings():\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:476\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    464\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    465\u001B[0m         map_nested(\n\u001B[1;32m    466\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m    473\u001B[0m     ]\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m num_proc \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m num_proc \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m<\u001B[39m parallel_min_length:\n\u001B[1;32m    475\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 476\u001B[0m         \u001B[43m_single_map_nested\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    477\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m hf_tqdm(iterable, disable\u001B[38;5;241m=\u001B[39mdisable_tqdm, desc\u001B[38;5;241m=\u001B[39mdesc)\n\u001B[1;32m    478\u001B[0m     ]\n\u001B[1;32m    479\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    480\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings():\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:385\u001B[0m, in \u001B[0;36m_single_map_nested\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {k: _single_map_nested((function, v, types, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m pbar}\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 385\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [_single_map_nested((function, v, types, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m pbar]\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    387\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m mapped\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:385\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {k: _single_map_nested((function, v, types, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m pbar}\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 385\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\u001B[43m_single_map_nested\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m pbar]\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    387\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m mapped\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:368\u001B[0m, in \u001B[0;36m_single_map_nested\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;66;03m# Singleton first to spare some computation\u001B[39;00m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, types):\n\u001B[0;32m--> 368\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_struct\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rank \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mget_verbosity() \u001B[38;5;241m<\u001B[39m logging\u001B[38;5;241m.\u001B[39mWARNING:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/download/download_manager.py:459\u001B[0m, in \u001B[0;36mDownloadManager._download\u001B[0;34m(self, url_or_filename, download_config)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_relative_path(url_or_filename):\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;66;03m# append the relative path to the base_path\u001B[39;00m\n\u001B[1;32m    458\u001B[0m     url_or_filename \u001B[38;5;241m=\u001B[39m url_or_path_join(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_base_path, url_or_filename)\n\u001B[0;32m--> 459\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mcached_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m out \u001B[38;5;241m=\u001B[39m tracked_str(out)\n\u001B[1;32m    461\u001B[0m out\u001B[38;5;241m.\u001B[39mset_origin(url_or_filename)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/file_utils.py:190\u001B[0m, in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, download_config, **download_kwargs)\u001B[0m\n\u001B[1;32m    186\u001B[0m     url_or_filename \u001B[38;5;241m=\u001B[39m strip_protocol(url_or_filename)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_url(url_or_filename):\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_from_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_etag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_etag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_url_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_url_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_desc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_desc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(url_or_filename):\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;66;03m# File, and it exists.\u001B[39;00m\n\u001B[1;32m    207\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m url_or_filename\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/file_utils.py:632\u001B[0m, in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001B[0m\n\u001B[1;32m    630\u001B[0m     ftp_get(url, temp_file)\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m scheme \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 632\u001B[0m     \u001B[43mfsspec_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_desc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    634\u001B[0m     http_get(\n\u001B[1;32m    635\u001B[0m         url,\n\u001B[1;32m    636\u001B[0m         temp_file\u001B[38;5;241m=\u001B[39mtemp_file,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    642\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdownload_desc,\n\u001B[1;32m    643\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/utils/file_utils.py:352\u001B[0m, in \u001B[0;36mfsspec_get\u001B[0;34m(url, temp_file, storage_options, desc)\u001B[0m\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET can be called with at most one path but was called with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpaths\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    341\u001B[0m callback \u001B[38;5;241m=\u001B[39m TqdmCallback(\n\u001B[1;32m    342\u001B[0m     tqdm_kwargs\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    343\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdesc\u001B[39m\u001B[38;5;124m\"\u001B[39m: desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    350\u001B[0m     }\n\u001B[1;32m    351\u001B[0m )\n\u001B[0;32m--> 352\u001B[0m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/fsspec/spec.py:900\u001B[0m, in \u001B[0;36mAbstractFileSystem.get_file\u001B[0;34m(self, rpath, lpath, callback, outfile, **kwargs)\u001B[0m\n\u001B[1;32m    898\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    899\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m data:\n\u001B[0;32m--> 900\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocksize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    901\u001B[0m     segment_len \u001B[38;5;241m=\u001B[39m outfile\u001B[38;5;241m.\u001B[39mwrite(data)\n\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m segment_len \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/fsspec/spec.py:1846\u001B[0m, in \u001B[0;36mAbstractBufferedFile.read\u001B[0;34m(self, length)\u001B[0m\n\u001B[1;32m   1843\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m length \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1844\u001B[0m     \u001B[38;5;66;03m# don't even bother calling fetch\u001B[39;00m\n\u001B[1;32m   1845\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1846\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fetch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1847\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(out)\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/fsspec/caching.py:189\u001B[0m, in \u001B[0;36mReadAheadCache._fetch\u001B[0;34m(self, start, end)\u001B[0m\n\u001B[1;32m    187\u001B[0m     part \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    188\u001B[0m end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize, end \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocksize)\n\u001B[0;32m--> 189\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetcher\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# new block replaces old\u001B[39;00m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m=\u001B[39m start\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mend \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:642\u001B[0m, in \u001B[0;36mHfFileSystemFile._fetch_range\u001B[0;34m(self, start, end)\u001B[0m\n\u001B[1;32m    631\u001B[0m headers \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrange\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbytes=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mend\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39m_build_hf_headers(),\n\u001B[1;32m    634\u001B[0m }\n\u001B[1;32m    635\u001B[0m url \u001B[38;5;241m=\u001B[39m hf_hub_url(\n\u001B[1;32m    636\u001B[0m     repo_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolved_path\u001B[38;5;241m.\u001B[39mrepo_id,\n\u001B[1;32m    637\u001B[0m     revision\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolved_path\u001B[38;5;241m.\u001B[39mrevision,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    640\u001B[0m     endpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mendpoint,\n\u001B[1;32m    641\u001B[0m )\n\u001B[0;32m--> 642\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mhttp_backoff\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_on_status_codes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m502\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m503\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m504\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m hf_raise_for_status(r)\n\u001B[1;32m    644\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:281\u001B[0m, in \u001B[0;36mhttp_backoff\u001B[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001B[0m\n\u001B[1;32m    278\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mseek(io_obj_initial_pos)\n\u001B[1;32m    280\u001B[0m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[0;32m--> 281\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m retry_on_status_codes:\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:725\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_redirects:\n\u001B[1;32m    723\u001B[0m     \u001B[38;5;66;03m# Redirect resolving generator.\u001B[39;00m\n\u001B[1;32m    724\u001B[0m     gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolve_redirects(r, request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 725\u001B[0m     history \u001B[38;5;241m=\u001B[39m [resp \u001B[38;5;28;01mfor\u001B[39;00m resp \u001B[38;5;129;01min\u001B[39;00m gen]\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    727\u001B[0m     history \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:725\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_redirects:\n\u001B[1;32m    723\u001B[0m     \u001B[38;5;66;03m# Redirect resolving generator.\u001B[39;00m\n\u001B[1;32m    724\u001B[0m     gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolve_redirects(r, request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 725\u001B[0m     history \u001B[38;5;241m=\u001B[39m [resp \u001B[38;5;28;01mfor\u001B[39;00m resp \u001B[38;5;129;01min\u001B[39;00m gen]\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    727\u001B[0m     history \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:266\u001B[0m, in \u001B[0;36mSessionRedirectMixin.resolve_redirects\u001B[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001B[0m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m req\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 266\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcert\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madapter_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m     extract_cookies_to_jar(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcookies, prepared_request, resp\u001B[38;5;241m.\u001B[39mraw)\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;66;03m# extract redirect url, if any, for the next loop\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:747\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    744\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m--> 747\u001B[0m     \u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/models.py:899\u001B[0m, in \u001B[0;36mResponse.content\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    897\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    898\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 899\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/requests/models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:628\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp):\n\u001B[0;32m--> 628\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    630\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m    631\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:567\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    564\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 567\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    569\u001B[0m         flush_decoder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/response.py:533\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m buffer\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    532\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/http/client.py:465\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    464\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[0;32m--> 465\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T21:42:38.427831Z",
     "start_time": "2024-11-19T21:42:38.398892Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8d703fe6ce1fa494",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T21:42:39.996147Z",
     "start_time": "2024-11-19T21:42:38.431134Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e3043e107e86d1e6",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[32, 1, 1, 1, 1, 128]}, size=[32, 12, 128, 128]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (6)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, num_epochs)\u001B[0m\n\u001B[1;32m     15\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, vocab_size), labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     20\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/diff-transformer/module/Diff_Transformer.py:39\u001B[0m, in \u001B[0;36mDifferentialTransformer.forward\u001B[0;34m(self, x, attention_mask)\u001B[0m\n\u001B[1;32m     36\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m attention_mask\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 39\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[1;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/diff-transformer/module/diff_layer.py:19\u001B[0m, in \u001B[0;36mDifferentialTransformerLayer.forward\u001B[0;34m(self, x, context, mask)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     18\u001B[0m     normed_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[0;32m---> 19\u001B[0m     attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormed_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attn_output)\n\u001B[1;32m     21\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m attn_output\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/diff-transformer/module/diff_attn.py:72\u001B[0m, in \u001B[0;36mDifferentialAttention.forward\u001B[0;34m(self, x, context, mask)\u001B[0m\n\u001B[1;32m     70\u001B[0m         mask \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     71\u001B[0m     mask \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 72\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[43mmask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     attn_diff \u001B[38;5;241m=\u001B[39m attn_diff\u001B[38;5;241m.\u001B[39mmasked_fill(mask \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-inf\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-inf\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     75\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(attn_diff, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expand(torch.LongTensor{[32, 1, 1, 1, 1, 128]}, size=[32, 12, 128, 128]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (6)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "430fcfce1c6c2af3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
